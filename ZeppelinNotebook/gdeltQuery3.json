{"paragraphs":[{"text":"%md\n## Querry 3\nHere we will try to resolve the following query:\n\nFor an input source (gkg.SourceCommonName), display themes, persons, locations, that are mentions in this article source. You will also display the number of article and the average tone for each theme/person and location. A aggregation on day, month and year should be possible.\n\nWe will use the table GKG to answer this query.\n\n","user":"anonymous","dateUpdated":"2020-01-23T09:31:21+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Querry 3</h2>\n<p>Here we will try to resolve the following query:</p>\n<p>For an input source (gkg.SourceCommonName), display themes, persons, locations, that are mentions in this article source. You will also display the number of article and the average tone for each theme/person and location. A aggregation on day, month and year should be possible.</p>\n<p>We will use the table GKG to answer this query.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1579771881149_1889034822","id":"20181212-102323_67420128","dateCreated":"2020-01-23T09:31:21+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:6036"},{"text":"%md Edit the interpreter spark (top right drop down menu) and add those two variables:\n```\nspark.jars.packages                         datastax:spark-cassandra-connector:2.4.0-s_2.11\nspark.cassandra.connection.host             private-ip-cassandra-node-1,private-ip-cassandra-node-2 \n```","user":"anonymous","dateUpdated":"2020-01-23T09:31:21+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Edit the interpreter spark (top right drop down menu) and add those two variables:</p>\n<pre><code>spark.jars.packages                         datastax:spark-cassandra-connector:2.4.0-s_2.11\nspark.cassandra.connection.host             private-ip-cassandra-node-1,private-ip-cassandra-node-2 \n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1579771881156_-918811558","id":"20200119-122507_1982817269","dateCreated":"2020-01-23T09:31:21+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6037"},{"text":"val AWS_ID = \"ASIAX7FDIQZH7OWIEFUU\"\nval AWS_KEY = \"8bD02bkzRHCg4WJMSmX3fYZ9hIrlctPLzmzKNgXP\"\nval AWS_TOKEN = \"FwoGZXIvYXdzELv//////////wEaDHLMYD4A8LUL6toLfSK/AQm6692/2iSpw1X+3II11M2oOQmJR1UCHyfYCf+I/sc0lCT2g11LfmHHY5RDWZUg5tPfVYk4LWbGtbh4fHmFpJgZ7tntlNrd9v8LK0IMSjZ+tDwm4jOYzGF3Zh2DcTz4vdp955JFEB9ujP3RkeZ4Isv3NBRFWC3cHQcV2ZUMhLNiMdeem9nmvOCxoHAag2pCPiDVM8OXOZ4coAACXWVHip/vjojMrQZG6aEs7ad5udRB3aui5WEzDWrqCEyPwP50KNjQpfEFMi36vsmvDnw+Z9sAj8G2i3s6GSn5sbP5t8807KVXEzbofkkti46S0VgJNxVYem0=\"\nval s3_name = \"projet-gdelt-2019\"\n\n\nsc.hadoopConfiguration.set(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\")\nsc.hadoopConfiguration.set(\"fs.s3a.access.key\", AWS_ID) // mettre votre ID du fichier credentials.csv\nsc.hadoopConfiguration.set(\"fs.s3a.secret.key\", AWS_KEY) // mettre votre secret du fichier credentials.csv\nsc.hadoopConfiguration.set(\"fs.s3a.session.token\", AWS_TOKEN)","user":"anonymous","dateUpdated":"2020-01-23T09:35:19+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"AWS_ID: String = ASIAX7FDIQZH7OWIEFUU\nAWS_KEY: String = 8bD02bkzRHCg4WJMSmX3fYZ9hIrlctPLzmzKNgXP\nAWS_TOKEN: String = FwoGZXIvYXdzELv//////////wEaDHLMYD4A8LUL6toLfSK/AQm6692/2iSpw1X+3II11M2oOQmJR1UCHyfYCf+I/sc0lCT2g11LfmHHY5RDWZUg5tPfVYk4LWbGtbh4fHmFpJgZ7tntlNrd9v8LK0IMSjZ+tDwm4jOYzGF3Zh2DcTz4vdp955JFEB9ujP3RkeZ4Isv3NBRFWC3cHQcV2ZUMhLNiMdeem9nmvOCxoHAag2pCPiDVM8OXOZ4coAACXWVHip/vjojMrQZG6aEs7ad5udRB3aui5WEzDWrqCEyPwP50KNjQpfEFMi36vsmvDnw+Z9sAj8G2i3s6GSn5sbP5t8807KVXEzbofkkti46S0VgJNxVYem0=\ns3_name: String = projet-gdelt-2019\n"}]},"apps":[],"jobName":"paragraph_1579771881156_1994412028","id":"20171217-230735_1688540039","dateCreated":"2020-01-23T09:31:21+0000","dateStarted":"2020-01-23T09:35:19+0000","dateFinished":"2020-01-23T09:36:03+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6038"},{"text":"%md Exportation of the GKG Table","user":"anonymous","dateUpdated":"2020-01-23T09:31:21+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Exportation of the GKG Table</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1579771881157_1962827583","id":"20200117-103749_1442620581","dateCreated":"2020-01-23T09:31:21+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6039"},{"text":"import org.apache.spark.input.PortableDataStream\nimport java.util.zip.ZipInputStream\nimport java.io.BufferedReader\nimport java.io.InputStreamReader\n\nval RDDgkg = sc.binaryFiles(\"s3://\" + s3_name + \"/20191201*.gkg.csv.zip,s3://\" + s3_name + \"/20191201*.translation.gkg.csv.zip\").\n   flatMap {  // decompresser les fichiers\n       case (name: String, content: PortableDataStream) =>\n          val zis = new ZipInputStream(content.open)\n          Stream.continually(zis.getNextEntry).\n            takeWhile{ case null => zis.close(); false\n            case _ => true }.\n            flatMap { _ =>\n                val br = new BufferedReader(new InputStreamReader(zis))\n                Stream.continually(br.readLine()).takeWhile(_ != null)\n            }\n    }\n    \nval dfgkg = RDDgkg.toDF.withColumn(\"GLOBALEVENTID\", split($\"value\", \"\\\\t\").getItem(0))\n.withColumn(\"GKGRECORDID\", split($\"value\", \"\\\\t\").getItem(0))\n.withColumn(\"DATE\", split($\"value\", \"\\\\t\").getItem(1))\n.withColumn(\"SourceCollectionIdentifier\", split($\"value\", \"\\\\t\").getItem(2))\n.withColumn(\"source_common_name\", split($\"value\", \"\\\\t\").getItem(3))\n.withColumn(\"DocumentIdentifier\", split($\"value\", \"\\\\t\").getItem(4))\n.withColumn(\"Counts\", split($\"value\", \"\\\\t\").getItem(5))\n.withColumn(\"V2Counts\", split($\"value\", \"\\\\t\").getItem(6))\n.withColumn(\"Themes\", split($\"value\", \"\\\\t\").getItem(7))\n.withColumn(\"V2Themes\", split($\"value\", \"\\\\t\").getItem(8))\n.withColumn(\"Locations\", split($\"value\", \"\\\\t\").getItem(9))\n.withColumn(\"V2Locations\", split($\"value\", \"\\\\t\").getItem(10))\n.withColumn(\"Persons\", split($\"value\", \"\\\\t\").getItem(11))\n.withColumn(\"V2Persons\", split($\"value\", \"\\\\t\").getItem(12))\n.withColumn(\"Organizations\", split($\"value\", \"\\\\t\").getItem(13))\n.withColumn(\"V2Organizations\", split($\"value\", \"\\\\t\").getItem(14))\n.withColumn(\"V2Tone\", split($\"value\", \"\\\\t\").getItem(15))\n.withColumn(\"Dates\", split($\"value\", \"\\\\t\").getItem(16))\n.withColumn(\"GCAM\", split($\"value\", \"\\\\t\").getItem(17))\n.withColumn(\"SharingImage\", split($\"value\", \"\\\\t\").getItem(18))\n.withColumn(\"RelatedImages\", split($\"value\", \"\\\\t\").getItem(19))\n.withColumn(\"SocialImageEmbeds\", split($\"value\", \"\\\\t\").getItem(20))\n.withColumn(\"SocialVideoEmbeds\", split($\"value\", \"\\\\t\").getItem(21))\n.withColumn(\"Quotations\", split($\"value\", \"\\\\t\").getItem(22))\n.withColumn(\"AllNames\", split($\"value\", \"\\\\t\").getItem(23))\n.withColumn(\"Amounts\", split($\"value\", \"\\\\t\").getItem(24))\n.withColumn(\"TranslationInfo\", split($\"value\", \"\\\\t\").getItem(25))\n.withColumn(\"Extras\", split($\"value\", \"\\\\t\").getItem(26))\n.drop(\"value\").\ncache","user":"anonymous","dateUpdated":"2020-01-23T09:35:38+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.input.PortableDataStream\nimport java.util.zip.ZipInputStream\nimport java.io.BufferedReader\nimport java.io.InputStreamReader\nRDDgkg: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[1] at flatMap at <console>:33\ndfgkg: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [GLOBALEVENTID: string, GKGRECORDID: string ... 26 more fields]\n"}]},"apps":[],"jobName":"paragraph_1579771881157_-1621026321","id":"20200117-103556_2139418060","dateCreated":"2020-01-23T09:31:21+0000","dateStarted":"2020-01-23T09:35:38+0000","dateFinished":"2020-01-23T09:36:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6040"},{"text":"%md Processing of the data","user":"anonymous","dateUpdated":"2020-01-23T09:31:21+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Processing of the data</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1579771881158_2095274101","id":"20200119-122641_344541628","dateCreated":"2020-01-23T09:31:21+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6041"},{"text":"import org.apache.spark.sql.functions._\n\n\nval df_article_by_theme = dfgkg\n    .select(\"GKGRECORDID\", \"source_common_name\", \"Themes\", \"V2Tone\", \"DATE\")\n    .withColumn(\"theme\", explode(split($\"Themes\", \";\")))\n    .filter(!($\"theme\".isNaN || $\"theme\".isNull || $\"theme\" === \"\"))\n    .withColumn(\"Tone\", substring_index($\"V2Tone\", \",\", 1))\n    .withColumn(\"year\", substring($\"DATE\", 0, 4))\n    .withColumn(\"month\", substring($\"DATE\", 5, 2))\n    .withColumn(\"day\", substring($\"DATE\", 7, 2))\n    .groupBy(\"source_common_name\",\"theme\", \"year\", \"month\", \"day\")\n    .agg(count($\"GKGRECORDID\").alias(\"num_article\"), \n        sum($\"tone\").alias(\"sum_tone\"))\n\nval df_article_by_person = dfgkg\n    .select(\"GKGRECORDID\", \"source_common_name\",\"Persons\", \"V2Tone\", \"DATE\")\n    .withColumn(\"person\", explode(split($\"Persons\", \";\")))\n    .filter(!($\"person\".isNaN || $\"person\".isNull || $\"person\" === \"\"))\n    .withColumn(\"Tone\", substring_index($\"V2Tone\", \",\", 1))\n    .withColumn(\"year\", substring($\"DATE\", 0, 4))\n    .withColumn(\"month\", substring($\"DATE\", 5, 2))\n    .withColumn(\"day\", substring($\"DATE\", 7, 2))\n    .groupBy(\"source_common_name\",\"person\", \"year\", \"month\", \"day\")\n    .agg(count($\"GKGRECORDID\").alias(\"num_article\"), \n        sum($\"tone\").alias(\"sum_tone\"))\n\nval df_article_by_location = dfgkg\n    .select(\"GKGRECORDID\", \"source_common_name\", \"V2Locations\", \"V2Tone\", \"DATE\")\n    .withColumn(\"Locations\", explode(split($\"V2Locations\", \";\")))\n    .filter(!($\"Locations\".isNaN || $\"Locations\".isNull || $\"Locations\" === \"\"))\n    .withColumn(\"location\", element_at(split($\"Locations\", \"#\"),2))\n    .withColumn(\"Tone\", substring_index($\"V2Tone\", \",\", 1))\n    .withColumn(\"year\", substring($\"DATE\", 0, 4))\n    .withColumn(\"month\", substring($\"DATE\", 5, 2))\n    .withColumn(\"day\", substring($\"DATE\", 7, 2))\n    .groupBy(\"source_common_name\",\"location\", \"year\", \"month\", \"day\")\n    .agg(count($\"GKGRECORDID\").alias(\"num_article\"), \n        sum($\"tone\").alias(\"sum_tone\"))","user":"anonymous","dateUpdated":"2020-01-23T09:36:52+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{"0":{"graph":{"mode":"table","height":325,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.functions._\ndf_article_by_theme: org.apache.spark.sql.DataFrame = [source_common_name: string, theme: string ... 5 more fields]\ndf_article_by_person: org.apache.spark.sql.DataFrame = [source_common_name: string, person: string ... 5 more fields]\ndf_article_by_location: org.apache.spark.sql.DataFrame = [source_common_name: string, location: string ... 5 more fields]\n"}]},"apps":[],"jobName":"paragraph_1579771881159_-1157452751","id":"20200117-102214_1513863359","dateCreated":"2020-01-23T09:31:21+0000","dateStarted":"2020-01-23T09:36:52+0000","dateFinished":"2020-01-23T09:36:54+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6042"},{"text":"df_article_by_theme.show(5, false)","user":"anonymous","dateUpdated":"2020-01-23T09:31:21+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-----------------------+----------------------------------+----+-----+---+-----------+------------------+\n|source_common_name     |theme                             |year|month|day|num_article|sum_tone          |\n+-----------------------+----------------------------------+----+-----+---+-----------+------------------+\n|shelbystar.com         |NATURAL_DISASTER_SNOWSTORMS       |2019|12   |01 |1          |-3.94321766561514 |\n|freemalaysiatoday.com  |WB_2473_DIPLOMACY_AND_NEGOTIATIONS|2019|12   |01 |6          |0.6179959575458933|\n|cbs46.com              |MEDIA_SOCIAL                      |2019|12   |01 |2          |-5.16355771830224 |\n|valdostadailytimes.com |EPU_CATS_MIGRATION_FEAR_FEAR      |2019|12   |01 |3          |3.38888608360579  |\n|thisislocallondon.co.uk|TAX_TERROR_GROUP_ISLAMIC_STATE    |2019|12   |01 |2          |-8.4475201845444  |\n+-----------------------+----------------------------------+----+-----+---+-----------+------------------+\nonly showing top 5 rows\n\n"}]},"apps":[],"jobName":"paragraph_1579771881159_-1641401600","id":"20200118-110415_12826907","dateCreated":"2020-01-23T09:31:21+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6043"},{"text":"df_article_by_person.orderBy($\"num_article\".desc).show(5, false)","user":"anonymous","dateUpdated":"2020-01-23T09:31:21+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----------------+---------------------------+----+-----+---+-------------+-------------------+\n|SourceCommonName|Person                     |Year|Month|Day|NumberArticle|SumTone            |\n+----------------+---------------------------+----+-----+---+-------------+-------------------+\n|iheart.com      |john adams                 |2018|12   |01 |19           |-17.902425425146365|\n|iheart.com      |jeb bush                   |2018|12   |01 |19           |-17.902425425146365|\n|iheart.com      |pauline robinson robin bush|2018|12   |01 |19           |-17.902425425146365|\n|iheart.com      |john quincy adams          |2018|12   |01 |19           |-17.902425425146365|\n|iheart.com      |george h w bush            |2018|12   |01 |19           |-17.902425425146365|\n+----------------+---------------------------+----+-----+---+-------------+-------------------+\nonly showing top 5 rows\n\n"}]},"apps":[],"jobName":"paragraph_1579771881160_-1457727357","id":"20200118-114007_983119573","dateCreated":"2020-01-23T09:31:21+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6044"},{"text":"df_article_by_location.orderBy($\"num_article\".desc).show(5, false)","user":"anonymous","dateUpdated":"2020-01-23T09:31:21+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---------------------+----------+----+-----+---+-------------+-------------------+\n|SourceCommonName     |Location  |Year|Month|Day|NumberArticle|SumTone            |\n+---------------------+----------+----+-----+---+-------------+-------------------+\n|english.vietnamnet.vn|Vietnam   |2018|12   |01 |214          |471.2886890418726  |\n|indiatimes.com       |India     |2018|12   |01 |188          |181.67684121789893 |\n|english.wafa.ps      |Israel    |2018|12   |01 |151          |-641.7885161287817 |\n|timesca.com          |Kazakhstan|2018|12   |01 |146          |380.3854970207309  |\n|globalsecurity.org   |Russia    |2018|12   |01 |139          |-428.36390570135995|\n+---------------------+----------+----+-----+---+-------------+-------------------+\nonly showing top 5 rows\n\n"}]},"apps":[],"jobName":"paragraph_1579771881160_973495263","id":"20200118-151738_2046098198","dateCreated":"2020-01-23T09:31:21+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6045"},{"text":"%sh hdfs dfs -rm -r  /user/zeppelin/article_by_person.parquet","user":"anonymous","dateUpdated":"2020-01-23T09:31:21+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579771881161_-907565351","id":"20200119-230811_221470241","dateCreated":"2020-01-23T09:31:21+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6046"},{"text":"import org.apache.spark.sql.SaveMode\n\ndf_article_by_theme\n  .write\n  .mode(SaveMode.Overwrite)\n  .parquet(\"s3://\" + s3_name + \"/article_by_theme.parquet/\")\n\ndf_article_by_person\n  .write\n  .mode(SaveMode.Overwrite)\n  .parquet(\"s3://\" + s3_name + \"/article_by_person.parquet/\")\n\ndf_article_by_location\n  .write\n  .mode(SaveMode.Overwrite)\n  .parquet(\"s3://\" + s3_name + \"/article_by_location.parquet/\")","user":"anonymous","dateUpdated":"2020-01-23T09:37:21+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579771881161_-858979931","id":"20200119-222844_1554684525","dateCreated":"2020-01-23T09:31:21+0000","dateStarted":"2020-01-23T09:37:21+0000","dateFinished":"2020-01-23T09:56:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6047","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.SaveMode\n"}]}},{"text":"%md Creation of Cassandra table ","user":"anonymous","dateUpdated":"2020-01-23T09:31:21+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Creation of Cassandra table</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1579771881162_-1507599064","id":"20200119-122701_546900147","dateCreated":"2020-01-23T09:31:21+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6048"},{"text":"import com.datastax.spark.connector.cql.CassandraConnector\nimport org.apache.spark.sql.cassandra._\n\n\nCassandraConnector(sc.getConf).withSessionDo { session =>\n  session.execute(\n    \"\"\"\n       CREATE KEYSPACE IF NOT EXISTS gdelt\n       WITH REPLICATION =\n       {'class': 'SimpleStrategy', 'replication_factor': 2 };\n    \"\"\")\n  session.execute(\n    \"\"\"\n       CREATE TABLE IF NOT EXISTS gdelt.article_by_theme (\n          source_common_name text,\n          year int,\n          month int,\n          day int,\n          theme text,\n          num_article int,\n          sum_tone int,\n          PRIMARY KEY (source_common_name, year, month, day, theme)\n        );\n    \"\"\"\n  )\n  session.execute(\n    \"\"\"\n      CREATE TABLE IF NOT EXISTS gdelt.article_by_person (\n          source_common_name text,\n          year int,\n          month int,\n          day int,\n          person text,\n          num_article int,\n          sum_tone int,\n          PRIMARY KEY (source_common_name, year, month, day, person)\n      );\n    \"\"\"\n  )\n  session.execute(\n    \"\"\"\n      CREATE TABLE IF NOT EXISTS gdelt.article_by_location (\n          source_common_name text,\n          year int,\n          month int,\n          day int,\n          location text,\n          num_article int,\n          sum_tone int,\n          PRIMARY KEY (source_common_name, year, month, day, location)\n      );\n    \"\"\"\n  )\n}","user":"anonymous","dateUpdated":"2020-01-23T09:31:21+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import com.datastax.spark.connector.cql.CassandraConnector\nimport org.apache.spark.sql.cassandra._\nres26: com.datastax.driver.core.ResultSet = ResultSet[ exhausted: true, Columns[]]\n"}]},"apps":[],"jobName":"paragraph_1579771881164_-1038614883","id":"20200118-155656_977328761","dateCreated":"2020-01-23T09:31:21+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6049"},{"text":"%md Import of the dataframe data in Cassandra","user":"anonymous","dateUpdated":"2020-01-23T09:31:21+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Import of the dataframe data in Cassandra</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1579771881166_1264503785","id":"20200119-122733_569981743","dateCreated":"2020-01-23T09:31:21+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6050"},{"text":"\nimport com.datastax.spark.connector.writer.WriteConf\nimport org.apache.spark.sql.SaveMode\n\ndf_article_by_theme.write\n  .cassandraFormat(\"article_by_theme\", \"gdelt\")\n  .mode(SaveMode.Append)\n  .save()\n\ndf_article_by_person.write\n  .cassandraFormat(\"article_by_person\", \"gdelt\")\n  .save()\n\ndf_article_by_location.write\n  .cassandraFormat(\"article_by_location\", \"gdelt\")\n  .save()","user":"anonymous","dateUpdated":"2020-01-23T09:37:18+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 39.0 failed 4 times, most recent failure: Lost task 1.3 in stage 39.0 (TID 1505, ip-172-31-46-39.ec2.internal, executor 12): java.io.InterruptedIOException: getFileStatus on s3a://projet-gdelt-2019/20181201053000.gkg.csv.zip: com.amazonaws.SdkClientException: Unable to execute HTTP request: Timeout waiting for connection from pool\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:125)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:101)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1571)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:521)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:790)\n\tat org.apache.spark.input.PortableDataStream.open(PortableDataStream.scala:183)\n\tat $anonfun$1.apply(<console>:79)\n\tat $anonfun$1.apply(<console>:77)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:212)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.amazonaws.SdkClientException: Unable to execute HTTP request: Timeout waiting for connection from pool\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleRetryableException(AmazonHttpClient.java:1175)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1121)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:770)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:744)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:726)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:686)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:668)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:532)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:512)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4926)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4872)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1321)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1295)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:904)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1553)\n\t... 29 more\nCaused by: org.apache.http.conn.ConnectionPoolTimeoutException: Timeout waiting for connection from pool\n\tat org.apache.http.impl.conn.PoolingHttpClientConnectionManager.leaseConnection(PoolingHttpClientConnectionManager.java:314)\n\tat org.apache.http.impl.conn.PoolingHttpClientConnectionManager$1.get(PoolingHttpClientConnectionManager.java:280)\n\tat sun.reflect.GeneratedMethodAccessor68.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat com.amazonaws.http.conn.ClientConnectionRequestFactory$Handler.invoke(ClientConnectionRequestFactory.java:70)\n\tat com.amazonaws.http.conn.$Proxy32.get(Unknown Source)\n\tat org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:190)\n\tat org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:186)\n\tat org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185)\n\tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)\n\tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:56)\n\tat com.amazonaws.http.apache.client.impl.SdkHttpClient.execute(SdkHttpClient.java:72)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1297)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1113)\n\t... 42 more\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2041)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2029)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2028)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2028)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2262)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2211)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2200)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n  at com.datastax.spark.connector.RDDFunctions.saveToCassandra(RDDFunctions.scala:36)\n  at org.apache.spark.sql.cassandra.CassandraSourceRelation.insert(CassandraSourceRelation.scala:76)\n  at org.apache.spark.sql.cassandra.DefaultSource.createRelation(DefaultSource.scala:86)\n  at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:156)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n  ... 70 elided\nCaused by: java.io.InterruptedIOException: getFileStatus on s3a://projet-gdelt-2019/20181201053000.gkg.csv.zip: com.amazonaws.SdkClientException: Unable to execute HTTP request: Timeout waiting for connection from pool\n  at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:125)\n  at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:101)\n  at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1571)\n  at org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:521)\n  at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:790)\n  at org.apache.spark.input.PortableDataStream.open(PortableDataStream.scala:183)\n  at $anonfun$1.apply(<console>:79)\n  at $anonfun$1.apply(<console>:77)\n  at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n  at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n  at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n  at scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:212)\n  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)\n  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n  at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n  at org.apache.spark.scheduler.Task.run(Task.scala:123)\n  at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n  ... 3 more\nCaused by: com.amazonaws.SdkClientException: Unable to execute HTTP request: Timeout waiting for connection from pool\n  at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleRetryableException(AmazonHttpClient.java:1175)\n  at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1121)\n  at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:770)\n  at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:744)\n  at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:726)\n  at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:686)\n  at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:668)\n  at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:532)\n  at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:512)\n  at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4926)\n  at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4872)\n  at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1321)\n  at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1295)\n  at org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:904)\n  at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1553)\n  ... 29 more\nCaused by: org.apache.http.conn.ConnectionPoolTimeoutException: Timeout waiting for connection from pool\n  at org.apache.http.impl.conn.PoolingHttpClientConnectionManager.leaseConnection(PoolingHttpClientConnectionManager.java:314)\n  at org.apache.http.impl.conn.PoolingHttpClientConnectionManager$1.get(PoolingHttpClientConnectionManager.java:280)\n  at sun.reflect.GeneratedMethodAccessor68.invoke(Unknown Source)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:498)\n  at com.amazonaws.http.conn.ClientConnectionRequestFactory$Handler.invoke(ClientConnectionRequestFactory.java:70)\n  at com.amazonaws.http.conn.$Proxy32.get(Unknown Source)\n  at org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:190)\n  at org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:186)\n  at org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185)\n  at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)\n  at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:56)\n  at com.amazonaws.http.apache.client.impl.SdkHttpClient.execute(SdkHttpClient.java:72)\n  at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1297)\n  at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1113)\n  ... 42 more\n"}]},"apps":[],"jobName":"paragraph_1579771881166_-114903969","id":"20200119-115156_978747205","dateCreated":"2020-01-23T09:31:21+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6051"},{"text":"    val article_by_theme_test = spark.read\n      .cassandraFormat(\"article_by_theme\", \"gdelt\")\n      .load()","user":"anonymous","dateUpdated":"2020-01-23T09:31:21+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"article_by_theme_test: org.apache.spark.sql.DataFrame = [source_common_name: string, year: int ... 5 more fields]\n"}]},"apps":[],"jobName":"paragraph_1579771881167_-1498348454","id":"20200119-123344_803864096","dateCreated":"2020-01-23T09:31:21+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6052"},{"text":"article_by_theme_test.count","user":"anonymous","dateUpdated":"2020-01-23T09:31:21+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res30: Long = 115408\n"}]},"apps":[],"jobName":"paragraph_1579771881168_2124437175","id":"20200119-124515_1351915171","dateCreated":"2020-01-23T09:31:21+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6053"},{"text":"article_by_theme_test.count","user":"anonymous","dateUpdated":"2020-01-23T09:31:21+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res28: Long = 115408\n"}]},"apps":[],"jobName":"paragraph_1579771881168_980839806","id":"20200119-124529_1790921434","dateCreated":"2020-01-23T09:31:21+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6054"},{"user":"anonymous","dateUpdated":"2020-01-23T09:31:21+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579771881169_863161593","id":"20200119-124917_1168310995","dateCreated":"2020-01-23T09:31:21+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6055"}],"name":"gdeltQuery3","id":"2F1P2Y6W3","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"sh:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}